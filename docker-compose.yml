version: "3.7"

services:
  zookeeper:
    image: "bitnami/zookeeper:latest"
    container_name: zookeeper
    networks:
      - data_network
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  spark-master:
    image: bitnami/spark:latest
    ports:
      - "7077:7077"
    networks:
      - data_network
    environment:
      - SPARK_MODE=master

  spark-worker:
    image: bitnami/spark:latest
    networks:
      - data_network
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

  kafka:
    image: "bitnami/kafka:latest"
    container_name: kafka
    networks:
      - data_network
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - "KAFKA_LISTENERS=PLAINTEXT://:9092"
      - "KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181"
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CREATE_TOPICS="kafka-nifi-src:1:1,kafka-nifi-dst:1:1,kafka-final:1:1"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper

  nifi:
    image: apache/nifi:latest
    container_name: nifi
    networks:
      - data_network
    ports:
      - "8443:8443"
    volumes:
      - ./nifi/templates:/opt/nifi/nifi-current/conf/templates
    environment:
      - SINGLE_USER_CREDENTIALS_USERNAME=${NIFI_USERNAME}
      - SINGLE_USER_CREDENTIALS_PASSWORD=${NIFI_PASSWORD}
      - NIFI_ZK_CONNECT_STRING="zookeeper:2181"

  kafka-producer:
    build:
      context: ./kafka/producer
      dockerfile: Dockerfile
    container_name: kafka-producer
    environment:
      BINANCE_WEBSOCKETS: "wss://stream.binance.com:9443/ws/btcusdt@kline_1m"
      TOPIC: kafka-nifi-src
      BOOTSTRAP_SERVER: "kafka:9092"
    networks:
      - data_network
    depends_on:
      - kafka
      - nifi

  kafka-consumer:
    build:
      context: ./kafka/consumer
      dockerfile: Dockerfile
    container_name: kafka-consumer
    environment:
      TOPIC: kafka-nifi-dst
      BOOTSTRAP_SERVER: "kafka:9092"
      PRODUCER_TOPIC: kafka-final
    networks:
      - data_network
    volumes:
      - ./kafka/consumer/app:/app
    depends_on:
      - kafka
      - kafka-producer
      - spark-master
      - spark-worker

  kafka-connect:
    build:
      context: kafka-connect
      dockerfile: Dockerfile
    depends_on:
      - kafka
      - elasticsearch
    ports:
      - "8083:8083"
    networks:
      - data_network
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      # Elasticsearch Sink Connector specific settings
      CONNECTOR_NAME: "elasticsearch-sink"
      ES_HOST: "elasticsearch:9200"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"  # Log levels
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"  # Consumer interceptors for monitoring
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"  # Producer interceptors for monitoring
      # Any other environment variables needed for your setup
    command: 
      - bash 
      - -c 
      - |
        /usr/local/bin/init-kafka-connect.sh &
        /etc/confluent/docker/run

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: elasticsearch
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
      - 9300:9300
    depends_on:
      - kafka
    networks:
      - data_network

  kibana:
    container_name: kibana
    image: docker.elastic.co/kibana/kibana:8.12.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch
    networks:
      - data_network

networks:
  data_network:

volumes:
  elasticsearch-data:
